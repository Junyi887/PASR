{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.io import loadmat\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn\n",
    "import cmocean\n",
    "import h5py\n",
    "CMAP = cmocean.cm.balance\n",
    "CMAP = seaborn.cm.icefire"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def energy_specturm(u,v):\n",
    "    # Script for the computation of 3D spectrum of the Total Kinetic Energy\n",
    "    # Adapted to the Taylor-Green vortex (TGV) problem.\n",
    "    # CREATED by FARSHAD NAVAH\n",
    "    # McGill University\n",
    "    # farshad.navah .a.t. mail.mcgill.ca\n",
    "    # 2018\n",
    "    # provided as is with no garantee.\n",
    "    # Please cite:\n",
    "    #    https://github.com/fanav/Energy_Spectrum\n",
    "    #    https://arxiv.org/abs/1809.03966\n",
    "\n",
    "    # -----------------------------------------------------------------\n",
    "    #  IMPORTS - ENVIRONMENT\n",
    "    # -----------------------------------------------------------------\n",
    "\n",
    "    import numpy as np\n",
    "    import matplotlib.pyplot as plt\n",
    "    import time\n",
    "    from math import sqrt\n",
    "\n",
    "    # -----------------------------------------------------------------\n",
    "    #  TGV QUANTS\n",
    "    # -----------------------------------------------------------------\n",
    "\n",
    "    # These quantities are to account for particular non-dimensionalizations of state variables.\n",
    "    # In general U0=1.\n",
    "    c  = sqrt(1.4);\n",
    "    Ma = 0.1;\n",
    "    U0 = 1.0; \n",
    "\n",
    "    #load the ascii file\n",
    "    data = np.stack((u,v),axis=1)\n",
    "\n",
    "    print (\"shape of data = \",data.shape)\n",
    "\n",
    "    localtime = time.asctime( time.localtime(time.time()) )\n",
    "    print (\"Reading files...localtime\",localtime, \"- END\\n\")\n",
    "\n",
    "    # -----------------------------------------------------------------\n",
    "    #  COMPUTATIONS\n",
    "    # -----------------------------------------------------------------\n",
    "    localtime = time.asctime( time.localtime(time.time()) )\n",
    "    print (\"Computing spectrum... \",localtime)\n",
    "\n",
    "    N = data.shape[-1]\n",
    "    M= data.shape[-2]\n",
    "    print(\"N =\",N)\n",
    "    print(\"M =\",M)\n",
    "    eps = 1e-16 # to void log(0)\n",
    "\n",
    "    U = data[:,0].mean(axis=0)\n",
    "    V = data[:,1].mean(axis=0)\n",
    "    # U = data[:,3].reshape(N,N,N)/U0\n",
    "    # V = data[:,4].reshape(N,N,N)/U0\n",
    "    # W = data[:,5].reshape(N,N,N)/U0\n",
    "\n",
    "    amplsU = abs(np.fft.fftn(U)/U.size)\n",
    "    amplsV = abs(np.fft.fftn(V)/V.size)\n",
    "    print(f\"amplsU.shape = {amplsU.shape}\")\n",
    "    # amplsW = abs(np.fft.fftn(W)/W.size)\n",
    "\n",
    "    EK_U  = amplsU**2\n",
    "    EK_V  = amplsV**2 \n",
    "    # EK_W  = amplsW**2 \n",
    "\n",
    "    EK_U = np.fft.fftshift(EK_U)\n",
    "    EK_V = np.fft.fftshift(EK_V)\n",
    "    # EK_W = np.fft.fftshift(EK_W)\n",
    "\n",
    "    sign_sizex = np.shape(EK_U)[0]\n",
    "    sign_sizey = np.shape(EK_U)[1]\n",
    "    # sign_sizez = np.shape(EK_U)[2]\n",
    "\n",
    "    box_sidex = sign_sizex\n",
    "    box_sidey = sign_sizey\n",
    "    # box_sidez = sign_sizez\n",
    "\n",
    "    # box_radius = int(np.ceil((np.sqrt((box_sidex)**2+(box_sidey)**2+(box_sidez)**2))/2.)+1)\n",
    "    box_radius = int(np.ceil((np.sqrt((box_sidex)**2+(box_sidey)**2))/2.)+1)\n",
    "    centerx = int(box_sidex/2)\n",
    "    centery = int(box_sidey/2)\n",
    "    # centerz = int(box_sidez/2)\n",
    "\n",
    "    print (\"box sidex     =\",box_sidex) \n",
    "    print (\"box sidey     =\",box_sidey) \n",
    "    # print (\"box sidez     =\",box_sidez)\n",
    "    print (\"sphere radius =\",box_radius )\n",
    "    print (\"centerbox     =\",centerx)\n",
    "    print (\"centerboy     =\",centery)\n",
    "    # print (\"centerboz     =\",centerz,\"\\n\" )\n",
    "                    \n",
    "    EK_U_avsphr = np.zeros(box_radius,)+eps ## size of the radius\n",
    "    EK_V_avsphr = np.zeros(box_radius,)+eps ## size of the radius\n",
    "    # EK_W_avsphr = np.zeros(box_radius,)+eps ## size of the radius\n",
    "\n",
    "    # for i in range(box_sidex):\n",
    "    # \tfor j in range(box_sidey):\n",
    "    # \t\tfor k in range(box_sidez):            \n",
    "    # \t\t\twn =  int(np.round(np.sqrt((i-centerx)**2+(j-centery)**2+(k-centerz)**2)))\n",
    "    # \t\t\tEK_U_avsphr[wn] = EK_U_avsphr [wn] + EK_U [i,j,k]\n",
    "    # \t\t\tEK_V_avsphr[wn] = EK_V_avsphr [wn] + EK_V [i,j,k]    \n",
    "    # \t\t\tEK_W_avsphr[wn] = EK_W_avsphr [wn] + EK_W [i,j,k]        \n",
    "    for i in range(box_sidex):\n",
    "        for j in range(box_sidey):          \n",
    "            wn =  int(np.round(np.sqrt((i-centerx)**2+(j-centery)**2)))\n",
    "            EK_U_avsphr[wn] = EK_U_avsphr [wn] + EK_U [i,j]\n",
    "            EK_V_avsphr[wn] = EK_V_avsphr [wn] + EK_V [i,j]     \n",
    "    EK_avsphr = 0.5*(EK_U_avsphr + EK_V_avsphr)\n",
    "                            \n",
    "\n",
    "    realsize = len(np.fft.rfft(U[:,0]))\n",
    "\n",
    "    TKEofmean_discrete = 0.5*(np.sum(U/U.size)**2+np.sum(V/V.size)**2)\n",
    "    TKEofmean_sphere   = EK_avsphr[0]\n",
    "\n",
    "    total_TKE_discrete = np.sum(0.5*(U**2+V**2))/(N*M) # average over whole domaon / divied by total pixel-value\n",
    "    total_TKE_sphere   = np.sum(EK_avsphr)\n",
    "\n",
    "    result_dict = {\n",
    "    \"Real Kmax\": realsize,\n",
    "    \"Spherical Kmax\": len(EK_avsphr),\n",
    "    \"KE of the mean velocity discrete\": TKEofmean_discrete,\n",
    "    \"KE of the mean velocity sphere\": TKEofmean_sphere,\n",
    "    \"Mean KE discrete\": total_TKE_discrete,\n",
    "    \"Mean KE sphere\": total_TKE_sphere\n",
    "    }\n",
    "    print(result_dict)\n",
    "    localtime = time.asctime( time.localtime(time.time()) )\n",
    "    print (\"Computing spectrum... \",localtime, \"- END \\n\")\n",
    "    return realsize, EK_avsphr,result_dict\n",
    "    fig = plt.figure()\n",
    "    plt.title(\"Kinetic Energy Spectrum\")\n",
    "    plt.xlabel(r\"k (wavenumber)\")\n",
    "    plt.ylabel(r\"TKE of the k$^{th}$ wavenumber\")\n",
    "\n",
    "    print(realsize)\n",
    "    plt.loglog(np.arange(0,realsize),((EK_avsphr[0:realsize] )),'k')\n",
    "    plt.loglog(np.arange(realsize,len(EK_avsphr),1),((EK_avsphr[realsize:] )),'k--')\n",
    "    axes = plt.gca()\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "************decay_turb************\n",
      "max w: 7.82, min w: -5.72\n",
      "mean w: -0.000029, std w: 1.1907\n",
      "max u: 1.5468, min u: -1.4452\n",
      "mean u: -0.0000, std u: 0.4451\n",
      "max v: 1.9174, min v: -1.4077\n",
      "mean v: 0.0000, std v: 0.4559\n",
      "************burger2d************\n",
      "max u: 2.7903, min u: -1.2097\n",
      "mean u: 0.8246, std u: 0.2483\n",
      "max v: 1.6623, min v: -2.1578\n",
      "mean v: -0.3140, std v: 0.3626\n",
      "************rbc************\n",
      "max w: 33.45, min w: -33.47\n",
      "mean w: -0.033876, std w: 4.5677\n",
      "max u: 0.6425, min u: -0.5935\n",
      "mean u: -0.0007, std u: 0.1942\n",
      "max v: 0.7396, min v: -0.6896\n",
      "mean v: 0.0000, std v: 0.1997\n"
     ]
    }
   ],
   "source": [
    "import h5py\n",
    "def get_data_scale(data_name):\n",
    "    data_info = {\"decay_turb\":['../Decay_Turbulence_small/train/Decay_turb_small_128x128_7202.h5', 0.02],\n",
    "                 \"burger2d\": [\"../Burgers_2D_small/train/Burgers2D_128x128_702.h5\",0.001],\n",
    "                 \"rbc\": [\"../RBC_small/train/RBC_small_165_s2.h5\",0.01]}\n",
    "    f = h5py.File(data_info[data_name][0],'r')\n",
    "    w = f['tasks']['vorticity'][()] if data_name != \"burger2d\" else None\n",
    "    u = f['tasks']['u'][()]\n",
    "    v = f['tasks']['v'][()]\n",
    "    print(f\"************{data_name}************\")\n",
    "    print(f\"max w: {np.max(w):.2f}, min w: {np.min(w):.2f}\") if data_name != \"burger2d\" else None\n",
    "    print(f\"mean w: {np.mean(w):4f}, std w: {np.std(w):.4f}\") if data_name != \"burger2d\" else None\n",
    "    print(f\"max u: {np.max(u):.4f}, min u: {np.min(u):.4f}\")\n",
    "    print(f\"mean u: {np.mean(u):.4f}, std u: {np.std(u):.4f}\")\n",
    "    print(f\"max v: {np.max(v):.4f}, min v: {np.min(v):.4f}\")\n",
    "    print(f\"mean v: {np.mean(v):.4f}, std v: {np.std(v):.4f}\")\n",
    "    f.close()\n",
    "    return None\n",
    "\n",
    "get_data_scale(\"decay_turb\")\n",
    "get_data_scale(\"burger2d\")\n",
    "get_data_scale(\"rbc\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import h5py\n",
    "def plot_data(data_name = \"burger2d\",row=10,col=10,timescale_factor =10 ,in_channel=2,vmin=-1,vmax=1):\n",
    "\n",
    "    data_info = {\"decay_turb\":['../Decay_Turbulence_small/train/Decay_turb_small_128x128_7202.h5', 0.02],\n",
    "                 \"burger2d\": [\"../Burgers_2D_small/train/Burgers2D_128x128_702.h5\",0.001],\n",
    "                 \"rbc\": [\"../RBC_small/train/RBC_small_165_s2.h5\",0.01]}\n",
    "    f = h5py.File(data_info[data_name][0],'r')\n",
    "    if data_name == \"rbc\":\n",
    "        print(f['scales/sim_time'][()])\n",
    "        t = int(f['scales/sim_time'][0])\n",
    "    else:\n",
    "        print(f['tasks']['t'][()])\n",
    "        t = np.ceil(f['tasks']['t'][0])\n",
    "    print(f['tasks']['u'].shape)\n",
    "    if data_name == \"burger2d\" and in_channel ==1:\n",
    "        raise ValueError(\"Burger2D doesnt have vorticity channel\")\n",
    "    else:\n",
    "        if in_channel == 1:\n",
    "            w = f['tasks']['vorticity'][()]\n",
    "            fig,axs = plt.subplots(row,col,figsize=(20,20))\n",
    "            i = 0\n",
    "            for ax in axs:\n",
    "                for a in ax:\n",
    "                    a.axis('off')\n",
    "                    a.imshow(w[i*timescale_factor],cmap=CMAP,vmin=vmin,vmax=vmax)\n",
    "                    a.set_title(f\"t={t + i*timescale_factor*data_info[data_name][1]:.4}\")\n",
    "                    i+=1\n",
    "        elif in_channel == 2:\n",
    "            u = f['tasks']['u'][()]\n",
    "            v = f['tasks']['v'][()]\n",
    "            fig,axs = plt.subplots(row,col,figsize=(20,20))\n",
    "            i = 0\n",
    "            for ax in axs:\n",
    "                for a in ax:\n",
    "                    a.axis('off')\n",
    "                    a.imshow(u[i*timescale_factor],cmap=CMAP,vmin=vmin[0],vmax=vmax[0])\n",
    "                    a.set_title(f\"t={t + i*timescale_factor*data_info[data_name][1]:.4}\")\n",
    "                    i+=1\n",
    "\n",
    "            fig,axs = plt.subplots(row,col,figsize=(20,20))\n",
    "            i = 0\n",
    "            for ax in axs:\n",
    "                for a in ax:\n",
    "                    a.axis('off')\n",
    "                    a.imshow(v[i*timescale_factor],cmap=CMAP,vmin=vmin[1],vmax=vmax[1])\n",
    "                    a.set_title(f\"t={t + i*timescale_factor*data_info[data_name][1]:.4}\")\n",
    "                    i+=1\n",
    "        elif in_channel == 3:\n",
    "            u = f['tasks']['u'][()]\n",
    "            v = f['tasks']['v'][()]\n",
    "            w = f['tasks']['vorticity'][()]\n",
    "            fig,axs = plt.subplots(row,col,figsize=(20,20))\n",
    "            i = 0\n",
    "            for ax in axs:\n",
    "                for a in ax:\n",
    "                    a.axis('off')\n",
    "                    a.imshow(u[i*timescale_factor],cmap=CMAP,vmin=vmin[0],vmax=vmax[0])\n",
    "                    a.set_title(f\"t={t + i*timescale_factor*data_info[data_name][1]:.4}\")\n",
    "                    i+=1\n",
    "\n",
    "            fig,axs = plt.subplots(row,col,figsize=(20,20))\n",
    "            i = 0\n",
    "            for ax in axs:\n",
    "                for a in ax:\n",
    "                    a.axis('off')\n",
    "                    a.imshow(v[i*timescale_factor],cmap=CMAP,vmin=vmin[1],vmax=vmax[1])\n",
    "                    a.set_title(f\"t={t + i*timescale_factor*data_info[data_name][1]:.4}\")\n",
    "                    i+=1\n",
    "            fig,axs = plt.subplots(row,col,figsize=(20,20))\n",
    "            i = 0\n",
    "            for ax in axs:\n",
    "                for a in ax:\n",
    "                    a.axis('off')\n",
    "                    a.imshow(w[i*timescale_factor],cmap=CMAP,vmin=vmin[2],vmax=vmax[2])\n",
    "                    a.set_title(f\"t={t + i*timescale_factor*data_info[data_name][1]:.4}\")\n",
    "                    i+=1\n",
    "    f.close()\n",
    "    return print(\"visualization done\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RBC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_data_scale(\"rbc\")\n",
    "plot_data(data_name = \"rbc\",timescale_factor =5 ,in_channel=1,vmin=-10,vmax=10,row=3,col=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Burgers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_data_scale(\"burger2d\")\n",
    "plot_data(data_name = \"burger2d\",timescale_factor =5 ,in_channel=2,vmin=[-1.2,-1.6],vmax=[2.8,1],row=10,col=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Decay Turb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_data_scale(\"decay_turb\")\n",
    "plot_data(data_name = \"decay_turb\",timescale_factor =5 ,in_channel=1,vmin=-5.7,vmax=7.82,row=10,col=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_test_matrix(cols:int, final_index:int):\n",
    "    # Calculate the number of rows based on the final index and number of columns\n",
    "    rows = (final_index + 1) // (cols - 1)\n",
    "    \n",
    "    # Check if an additional row is needed to reach the final index\n",
    "    if (final_index + 1) % (cols - 1) != 0:\n",
    "        rows += 1\n",
    "    \n",
    "    # Initialize a matrix filled with zeros\n",
    "    matrix = np.zeros((rows, cols),dtype=int)\n",
    "    \n",
    "    # Fill the matrix according to the specified pattern\n",
    "    current_value = 0\n",
    "    for i in range(rows):\n",
    "        for j in range(cols):\n",
    "            if current_value <= final_index:\n",
    "                matrix[i, j] = current_value\n",
    "                current_value += 1\n",
    "        current_value -= 1  # Repeat the last element in the next row\n",
    "                \n",
    "    return matrix[:-1,:]\n",
    "\n",
    "import torch\n",
    "import torchvision.transforms as transforms\n",
    "from PIL import Image, ImageFilter\n",
    "def get_test_data(data_name,timescale_factor = 10,num_snapshot = 10,in_channel=1,upscale_factor=4):\n",
    "    data_info = {\"decay_turb\":['../Decay_Turbulence_small/train/Decay_turb_small_128x128_79.h5', 0.02],\n",
    "                 \"burger2d\": [\"../Burgers_2D_small/test/Burgers2D_128x128_79.h5\",0.001],\n",
    "                 \"rbc\": [\"../RBC_small/test/RBC_small_33_s2.h5\",0.01]}\n",
    "    with h5py.File(data_info[data_name][0],'r') as f:\n",
    "        w_truth = f['tasks']['vorticity'][()]\n",
    "        u_truth = f['tasks']['u'][()]\n",
    "        v_truth = f['tasks']['v'][()]\n",
    "    final_index = (u_truth.shape[0]-1)//timescale_factor\n",
    "    idx_matrix = generate_test_matrix(num_snapshot +1 , final_index)*timescale_factor    \n",
    "    print(idx_matrix)\n",
    "    if in_channel ==1:\n",
    "        hr_input = w_truth[idx_matrix[:,0]]\n",
    "        hr_target = w_truth[idx_matrix[:,:]]\n",
    "    elif in_channel ==2:\n",
    "        hr_input = np.stack((u_truth[idx_matrix[:,0]],v_truth[idx_matrix[:,0]]),axis=1)\n",
    "        hr_target = np.stack((u_truth[idx_matrix[:,:]],v_truth[idx_matrix[:,:]]),axis=2)\n",
    "    elif in_channel ==3:\n",
    "        hr_input = np.stack((w_truth[idx_matrix[:,0]],u_truth[idx_matrix[:,0]],v_truth[idx_matrix[:,0]]),axis=1)\n",
    "        hr_target = np.stack((w_truth[idx_matrix[:,:]],u_truth[idx_matrix[:,:]],v_truth[idx_matrix[:,:]]),axis=2)\n",
    "    # to low resolution\n",
    "    print(hr_target.shape)\n",
    "    transform = torch.from_numpy\n",
    "    img_shape_x = hr_input.shape[-2]\n",
    "    img_shape_y = hr_input.shape[-1]\n",
    "    input_transform = transforms.Resize((int(img_shape_x/upscale_factor),int(img_shape_y/upscale_factor)),Image.BICUBIC,antialias=False)\n",
    "    \n",
    "    # to tensor\n",
    "    lr_input_tensor = input_transform(transform(hr_input))\n",
    "    hr_target_tensor = transform(hr_target)\n",
    "    lr_input_tensor = lr_input_tensor.unsqueeze(1) if in_channel ==1 else lr_input_tensor\n",
    "    hr_target_tensor = hr_target_tensor.unsqueeze(2) if in_channel ==1 else hr_target_tensor\n",
    "    lr_input = lr_input_tensor.numpy()\n",
    "    # print(idx_matrix[:,0])\n",
    "    return lr_input,hr_target,lr_input_tensor,hr_target_tensor\n",
    "\n",
    "# lr_input,hr_target,lr_input_tensor,hr_target_tensor = get_test_data(\"rbc\",timescale_factor = 5,num_snapshot = 20,in_channel=3,upscale_factor=4)\n",
    "\n",
    "\n",
    "\n",
    "from src.models import *\n",
    "def get_prediction(model_dic,lr_input_tensor,hr_target_tensor,scale_factor,in_channels,task_dt,n_snapshot,ode_step):\n",
    "    model = PASR_MLP(upscale=scale_factor, in_chans=in_channels, img_size=256, window_size=8, depths=[6, 6, 6, 6], embed_dim=60, num_heads=[6, 6, 6, 6], mlp_ratio=2, upsampler=\"pixelshuffle\", resi_conv='1conv',mean=[0],std=[1])\n",
    "    model = torch.nn.DataParallel(model).cuda()\n",
    "    checkpoint = torch.load(model_dic)\n",
    "    model_dic = checkpoint['model_state_dict']\n",
    "    model.load_state_dict(model_dic)\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        pred0 = model(lr_input_tensor.float().cuda(),task_dt = task_dt,n_snapshot = n_snapshot,ode_step = ode_step,time_evol = False)\n",
    "        pred = model(lr_input_tensor.float().cuda(),task_dt = task_dt,n_snapshot = n_snapshot,ode_step = ode_step,time_evol = True)\n",
    "        pred = torch.cat((pred0,pred),dim=1)\n",
    "    ## add RFNE RINE PSNR SSIM in the future \n",
    "    return pred.detach().cpu().numpy()\n",
    "\n",
    "# model_dic = \"results/PASR_MLP_small_data_rbc_small_crop_size_256_ode_step_8_ode_method_Euler_task_dt_4_num_snapshots_20_upscale_factor_4_timescale_factor_5_loss_type_L1_lamb_0.3_lr_0.0003_gamma_0.95_normalizaiton_Falsetensor([[736]]).pt\"\n",
    "# pred = get_prediction(model_dic,lr_input_tensor,hr_target_tensor,scale_factor=4,in_channels=3,task_dt=4,n_snapshot=20,ode_step=8)\n",
    "\n",
    "# transform = torch.from_numpy\n",
    "# upscale_factor = 4\n",
    "# img_shape_x = 128\n",
    "# img_shape_y = 128\n",
    "# input_transform = transforms.Resize((int(img_shape_x/upscale_factor),int(img_shape_y/upscale_factor)),Image.BICUBIC,antialias=False)\n",
    "# uv_tensor = transform(uv)\n",
    "# w_lr = input_transform(uv_tensor)\n",
    "\n",
    "# import torch\n",
    "# import torch.nn as nn\n",
    "# import torch.nn.functional as F\n",
    "# checkpoint = torch.load(\"results/PASR_MLP_small_data_Burger2D_small_crop_size_256_ode_step_8_ode_method_Euler_task_dt_4_num_snapshots_20_upscale_factor_4_timescale_factor_10_loss_type_L1_lamb_1.0_lr_0.000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Energy Specturm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[   0    5   10   15   20   25   30   35   40   45   50   55   60   65\n",
      "    70   75   80   85   90   95  100]\n",
      " [ 100  105  110  115  120  125  130  135  140  145  150  155  160  165\n",
      "   170  175  180  185  190  195  200]\n",
      " [ 200  205  210  215  220  225  230  235  240  245  250  255  260  265\n",
      "   270  275  280  285  290  295  300]\n",
      " [ 300  305  310  315  320  325  330  335  340  345  350  355  360  365\n",
      "   370  375  380  385  390  395  400]\n",
      " [ 400  405  410  415  420  425  430  435  440  445  450  455  460  465\n",
      "   470  475  480  485  490  495  500]\n",
      " [ 500  505  510  515  520  525  530  535  540  545  550  555  560  565\n",
      "   570  575  580  585  590  595  600]\n",
      " [ 600  605  610  615  620  625  630  635  640  645  650  655  660  665\n",
      "   670  675  680  685  690  695  700]\n",
      " [ 700  705  710  715  720  725  730  735  740  745  750  755  760  765\n",
      "   770  775  780  785  790  795  800]\n",
      " [ 800  805  810  815  820  825  830  835  840  845  850  855  860  865\n",
      "   870  875  880  885  890  895  900]\n",
      " [ 900  905  910  915  920  925  930  935  940  945  950  955  960  965\n",
      "   970  975  980  985  990  995 1000]\n",
      " [1000 1005 1010 1015 1020 1025 1030 1035 1040 1045 1050 1055 1060 1065\n",
      "  1070 1075 1080 1085 1090 1095 1100]\n",
      " [1100 1105 1110 1115 1120 1125 1130 1135 1140 1145 1150 1155 1160 1165\n",
      "  1170 1175 1180 1185 1190 1195 1200]\n",
      " [1200 1205 1210 1215 1220 1225 1230 1235 1240 1245 1250 1255 1260 1265\n",
      "  1270 1275 1280 1285 1290 1295 1300]\n",
      " [1300 1305 1310 1315 1320 1325 1330 1335 1340 1345 1350 1355 1360 1365\n",
      "  1370 1375 1380 1385 1390 1395 1400]\n",
      " [1400 1405 1410 1415 1420 1425 1430 1435 1440 1445 1450 1455 1460 1465\n",
      "  1470 1475 1480 1485 1490 1495 1500]\n",
      " [1500 1505 1510 1515 1520 1525 1530 1535 1540 1545 1550 1555 1560 1565\n",
      "  1570 1575 1580 1585 1590 1595 1600]\n",
      " [1600 1605 1610 1615 1620 1625 1630 1635 1640 1645 1650 1655 1660 1665\n",
      "  1670 1675 1680 1685 1690 1695 1700]\n",
      " [1700 1705 1710 1715 1720 1725 1730 1735 1740 1745 1750 1755 1760 1765\n",
      "  1770 1775 1780 1785 1790 1795 1800]\n",
      " [1800 1805 1810 1815 1820 1825 1830 1835 1840 1845 1850 1855 1860 1865\n",
      "  1870 1875 1880 1885 1890 1895 1900]]\n",
      "(19, 21, 3, 256, 64)\n",
      "(19, 21, 3, 256, 64)\n",
      "(19, 21, 3, 256, 64)\n"
     ]
    }
   ],
   "source": [
    "import h5py\n",
    "model_dic = \"results/PASR_MLP_small_data_rbc_small_crop_size_256_ode_step_8_ode_method_Euler_task_dt_4_num_snapshots_20_upscale_factor_4_timescale_factor_5_loss_type_L1_lamb_0.3_lr_0.0003_gamma_0.95_normalizaiton_Falsetensor([[736]]).pt\"\n",
    "lr_input,hr_target,lr_input_tensor,hr_target_tensor = get_test_data(\"rbc\",timescale_factor = 5,num_snapshot = 20,in_channel=3,upscale_factor=4)\n",
    "pred = get_prediction(model_dic,lr_input_tensor,hr_target_tensor,scale_factor=4,in_channels=3,task_dt=4,n_snapshot=20,ode_step=8)\n",
    "u_truth = hr_target[:,:,1]\n",
    "v_truth = hr_target[:,:,2]\n",
    "u_pred = pred[:,:,1]\n",
    "v_pred = pred[:,:,2] \n",
    "\n",
    "def plot_energy_specturm(u_truth,v_truth,u_pred,v_pred):\n",
    "\n",
    "    realsize_truth, EK_avsphr_truth,result_dict_truth = energy_specturm(u_truth,v_truth)\n",
    "    realsize_pred, EK_avsphr_pred,result_dict_pred = energy_specturm(u_pred,v_pred)\n",
    "\n",
    "    return "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import h5py\n",
    "\n",
    "    \n",
    "f = h5py.File('../../Burgers_2D_small/train/Burgers2D_128x128_161.h5','r')\n",
    "u = f['tasks']['u'][()]\n",
    "v = f['tasks']['v'][()]\n",
    "energy_specturm(u,v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import h5py\n",
    "f = h5py.File('../../Decay_Turbulence_small/train/Decay_turb_small_128x128_7202.h5','r')\n",
    "u = f['tasks']['u'][()]\n",
    "v = f['tasks']['v'][()]\n",
    "energy_specturm(u,v)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vorticity Correlation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_vorticity_correlation(data_name,pred,reference_data):\n",
    "    data_info = {\"decay_turb\":['../../Decay_Turbulence_small/train/Decay_turb_small_128x128_79.h5', 0.02],\n",
    "                 \"burger2d\": [\"../../Burgers_2D_small/test/Burgers2D_128x128_79.h5\",0.001],\n",
    "                 \"rbc\": [\"../../RBC_small/test/RBC_small_33_s2.h5\",0.01]}\n",
    "    f = h5py.File(data_info[data_name][0],'r')\n",
    "    w_truth = f['tasks']['vorticity'][()]\n",
    "    w_pred = pred\n",
    "    for t in range(w_truth.shape[0]):\n",
    "        ref_flat = reference_data[t].flatten()\n",
    "        gen_flat = generative_data[t].flatten()\n",
    "        corr, _ = pearsonr(ref_flat, gen_flat)\n",
    "        correlations[t] = corr\n",
    "    fig,axs = plt.subplots(1,1,figsize=(20,10))\n",
    "    axs.plot(np.arange(0,w_truth.shape[0]),correlations)\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.stats import pearsonr\n",
    "from scipy.ndimage import zoom\n",
    "\n",
    "# Simulate your reference and generative data\n",
    "reference_data = np.random.rand(1000, 256, 256)\n",
    "generative_data = np.random.rand(1000, 256, 256)\n",
    "generative_data = reference_data\n",
    "# Rescale generative_data to match the shape of reference_data\n",
    "# You can also downsample reference_data to match the shape of generative_data\n",
    "\n",
    "\n",
    "# Initialize an array to store correlation values\n",
    "correlations = np.zeros(1000)\n",
    "\n",
    "# Loop through each time-step to calculate correlation\n",
    "for t in range(1000):\n",
    "    ref_flat = reference_data[t].flatten()\n",
    "    gen_flat = generative_data[t].flatten()\n",
    "    corr, _ = pearsonr(ref_flat, gen_flat)\n",
    "    correlations[t] = corr\n",
    "\n",
    "# Now, `correlations` contains the Pearson correlation coefficients for each time-step\n",
    "plt.plot(correlations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = h5py.File('rbc_321_256_s9.h5','r')\n",
    "print(f['scales/sim_time'][()])\n",
    "w = f['tasks']['vorticity'][()]\n",
    "\n",
    "f.close()\n",
    "f = h5py.File('RBC_small/train/RBC_small_9217_s2.h5','r')\n",
    "w2 = f['tasks']['vorticity'][()]\n",
    "f.close()\n",
    "\n",
    "print(w.shape)\n",
    "print(w2.shape)\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Generate placeholder data\n",
    "u_x = w.reshape(-1)\n",
    "u_y = w2.reshape(-1)\n",
    "\n",
    "# Calculate the joint histogram\n",
    "histogram2D, xedges, yedges = np.histogram2d(u_x, u_y, bins=50, density=True)\n",
    "\n",
    "# Plotting the joint PDF using a heatmap\n",
    "plt.figure(figsize=(10, 8))\n",
    "plt.imshow(histogram2D, interpolation='nearest', origin='lower',\n",
    "           extent=[xedges[0], xedges[-1], yedges[0], yedges[-1]],\n",
    "           aspect='auto', cmap='viridis')\n",
    "plt.colorbar(label='Probability Density')\n",
    "plt.xlabel('u_x Velocity')\n",
    "plt.ylabel('u_y Velocity')\n",
    "plt.title('Joint Probability Density Function of 2D Fluid Flow Velocity')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# import seaborn as sns\n",
    "# prep_data =w.reshape(w.shape[0],-1)\n",
    "# prep_data_hat = w2.reshape(w2.shape[0],-1)\n",
    "# f, ax = plt.subplots(1)\n",
    "# sns.distplot(prep_data, hist=False, kde=True, label='Original')\n",
    "# sns.distplot(prep_data_hat, hist=False, kde=True, label='New')\n",
    "# ax.set_xscale('log')\n",
    "# ax.set_xbound(-1e-2,1e2)\n",
    "# # Plot formatting\n",
    "# plt.legend()\n",
    "# plt.xlabel('Data Value')\n",
    "# plt.ylabel('Data Density Estimate')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
